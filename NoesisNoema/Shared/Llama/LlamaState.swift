import Foundation

#if BRIDGE_TEST
// Minimal ModelManager stub for CLI builds if not included in target
@MainActor
class _CLI_ModelManager {
    static let shared = _CLI_ModelManager()
    private var mockEngine: _MockInferenceEngine? = nil

    func loadModel(id: String) async throws {
        mockEngine = _MockInferenceEngine()
    }

    func getCurrentEngine() -> _MockInferenceEngine? {
        return mockEngine
    }
}

// Minimal InferenceEngine stub for CLI
actor _MockInferenceEngine {
    func generate(prompt: String, maxTokens: Int32) async throws -> String {
        return "[BRIDGE_TEST stub response]"
    }
}

typealias ModelManager = _CLI_ModelManager
#endif

struct Model: Identifiable {
    var id = UUID()
    var name: String
    var url: String
    var filename: String
    var status: String?
}

@MainActor
class LlamaState: ObservableObject {
    @Published var messageLog = ""
    @Published var cacheCleared = false
    @Published var downloadedModels: [Model] = []
    @Published var undownloadedModels: [Model] = []
    let NS_PER_S = 1_000_000_000.0

    private var llamaContext: LlamaContext?
    private var defaultModelUrl: URL? {
        // „Éó„É©„ÉÉ„Éà„Éï„Ç©„Éº„É†ÂÑ™ÂÖà: iOS„ÅØJan„ÄÅmacOS„ÅØLlama3-8B
        #if os(macOS)
        let primaryFile = "llama3-8b.gguf"
        let secondaryFile = "Jan-v1-4B-Q4_K_M.gguf"
        #else
        let primaryFile = "Jan-v1-4B-Q4_K_M.gguf"
        let secondaryFile = "llama3-8b.gguf"
        #endif
        func findInBundle(_ file: String) -> URL? {
            let nameNoExt = (file as NSString).deletingPathExtension
            let ext = (file as NSString).pathExtension
            let subs = [nil, "Models", "Resources/Models", "Resources"]
            for sub in subs {
                if let url = Bundle.main.url(forResource: nameNoExt, withExtension: ext, subdirectory: sub) {
                    return url
                }
            }
            return nil
        }
        if let url = findInBundle(primaryFile) { return url }
        if let url = findInBundle(secondaryFile) { return url }
        // Êóß„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        return Bundle.main.url(forResource: "ggml-model", withExtension: "gguf", subdirectory: "models")
    }

    struct SamplingConfig {
        var temp: Float
        var topK: Int32
        var topP: Float
        var seed: UInt64
        var nLen: Int32
    }
    enum Preset: String { case factual, balanced, creative, json, code }

    private var pendingConfig: SamplingConfig = SamplingConfig(temp: 0.5, topK: 60, topP: 0.9, seed: 1234, nLen: 512)

    // Êñ∞Ë¶è: ModelManagerÁµ±ÂêàÔºàÊó¢Â≠ò„Ç≥„Éº„Éâ„Å®„ÅÆ‰∫íÊèõÊÄß„Çí‰øùÊåÅÔºâ
    private let modelManager = ModelManager.shared
    private var useModelManager = false // „Éï„É©„Ç∞„ÅßÊñ∞ÊóßAPIÂàá„ÇäÊõø„Åà

    init() {
        // „É©„É≥„Çø„Ç§„É†„Ç¨„Éº„ÉâÔºàÂ£ä„Çå„ÅüFW„ÇíÊó©ÊúüÊ§úÁü•Ôºâ
        if let err = LlamaRuntimeCheck.ensureLoadable() {
            let msg = "[LlamaRuntimeCheck] \(err)"
            messageLog += msg + "\n"
            SystemLog().logEvent(event: msg)
        }
        loadModelsFromDisk()
        loadDefaultModels()
    }

    // Êñ∞Ë¶è: ModelManagerÁµåÁî±„Åß„É¢„Éá„É´„Çí„É≠„Éº„Éâ
    func loadModelViaManager(id: String) async throws {
        try await modelManager.loadModel(id: id)
        useModelManager = true
        messageLog += "Loaded model via ModelManager: \(id)\n"
        SystemLog().logEvent(event: "[LlamaState] Using ModelManager for model: \(id)")
    }

    // Êñ∞Ë¶è: ModelManagerÁµåÁî±„ÅÆÊé®Ë´ñ
    func completeViaManager(text: String, maxTokens: Int32 = 512) async -> String {
        guard useModelManager else {
            return await complete(text: text) // „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        }

        guard let engine = modelManager.getCurrentEngine() else {
            messageLog += "ERROR: No model loaded in ModelManager\n"
            return ""
        }

        messageLog += "USER: \(text)\n"

        do {
            let result = try await engine.generate(prompt: text, maxTokens: maxTokens)
            let normalized = normalizeOutput(result)
            messageLog += "ASSISTANT: \(normalized)\n"
            return normalized
        } catch {
            let err = "Generation failed: \(error.localizedDescription)"
            messageLog += err + "\n"
            return ""
        }
    }

    // „Éó„É™„Çª„ÉÉ„Éà‚ÜíSamplingConfig Â§âÊèõ
    private func config(for preset: Preset) -> SamplingConfig {
        switch preset {
        case .factual:
            return SamplingConfig(temp: 0.2, topK: 40, topP: 0.85, seed: 1234, nLen: 384)
        case .balanced:
            return SamplingConfig(temp: 0.5, topK: 60, topP: 0.9, seed: 1234, nLen: 512)
        case .creative:
            return SamplingConfig(temp: 0.9, topK: 100, topP: 0.95, seed: 1234, nLen: 768)
        case .json:
            return SamplingConfig(temp: 0.2, topK: 40, topP: 0.9, seed: 1234, nLen: 512)
        case .code:
            return SamplingConfig(temp: 0.3, topK: 50, topP: 0.9, seed: 1234, nLen: 640)
        }
    }

    // „Ç§„É≥„ÉÜ„É≥„ÉàÊ§úÂá∫ÔºàÁ∞°ÊòìÔºâ
    private func detectIntent(prompt: String) -> Preset {
        let p = prompt.lowercased()
        if p.contains("context:") || p.contains("reference:") || p.contains("Ë≥áÊñô:") { return .factual }
        if p.contains("json") || p.contains("return json") || p.contains("Âá∫Âäõ„ÅØjson") || p.trimmingCharacters(in: .whitespacesAndNewlines).hasPrefix("{") { return .json }
        if p.contains("```") || p.contains("code") || p.contains("swift") || p.contains("python") || p.contains("Èñ¢Êï∞") || p.contains("„Ç≥„Éº„Éâ") { return .code }
        if p.contains("story") || p.contains("poem") || p.contains("creative") || p.contains("„Ç¢„Ç§„Éá„Ç¢") { return .creative }
        return .balanced
    }

    // „É¢„Éá„É´ÂêçÔºã„Ç§„É≥„ÉÜ„É≥„Éà„ÅßËá™Âãï„Éó„É™„Çª„ÉÉ„ÉàÈÅ∏Êäû
    func autoSelectPreset(modelFileName: String, prompt: String) -> Preset {
        var intent = detectIntent(prompt: prompt)
        let fn = modelFileName.lowercased()
        if fn.contains("jan") || fn.contains("qwen") {
            if intent == .balanced { intent = .factual }
        } else if fn.contains("llama3") {
            if intent == .factual { intent = .balanced }
        } else if fn.contains("mistral") || fn.contains("phi") || fn.contains("tinyllama") {
            if intent == .creative { intent = .balanced }
        }
        return intent
    }

    // Ë®≠ÂÆö„ÅÆÂèçÊò†ÔºàLlamaContext„Å∏Ôºâ
    private func applyConfigToContext() async {
        guard let llamaContext else { return }
        await llamaContext.configure_sampling(temp: pendingConfig.temp, top_k: pendingConfig.topK, top_p: pendingConfig.topP, seed: pendingConfig.seed)
        await llamaContext.set_n_len(pendingConfig.nLen)
    }

    // Â§ñÈÉ®ÂÖ¨Èñã: „Éó„É™„Çª„ÉÉ„ÉàÈÅ©Áî®
    func setPreset(_ name: String) async {
        if let p = Preset(rawValue: name) {
            pendingConfig = config(for: p)
            await applyConfigToContext()
        }
    }

    // Â§ñÈÉ®ÂÖ¨Èñã: Áõ¥Êé•Ë®≠ÂÆö
    func configure(temp: Float? = nil, topK: Int32? = nil, topP: Float? = nil, seed: UInt64? = nil, nLen: Int32? = nil) async {
        pendingConfig = SamplingConfig(
            temp: temp ?? pendingConfig.temp,
            topK: topK ?? pendingConfig.topK,
            topP: topP ?? pendingConfig.topP,
            seed: seed ?? pendingConfig.seed,
            nLen: nLen ?? pendingConfig.nLen
        )
        await applyConfigToContext()
    }

    // Â§ñÈÉ®ÂÖ¨Èñã: verbose „Éñ„É™„ÉÉ„Ç∏
    func setVerbose(_ on: Bool) async {
        guard let llamaContext else { return }
        await llamaContext.set_verbose(on)
    }

    private func loadModelsFromDisk() {
        do {
            let documentsURL = getDocumentsDirectory()
            let modelURLs = try FileManager.default.contentsOfDirectory(at: documentsURL, includingPropertiesForKeys: nil, options: [.skipsHiddenFiles, .skipsSubdirectoryDescendants])
            for modelURL in modelURLs {
                let modelName = modelURL.deletingPathExtension().lastPathComponent
                downloadedModels.append(Model(name: modelName, url: "", filename: modelURL.lastPathComponent, status: "downloaded"))
            }
        } catch {
            print("Error loading models from disk: \(error)")
        }
    }

    private func loadDefaultModels() {
        do {
            try loadModel(modelUrl: defaultModelUrl)
        } catch {
            messageLog += "Error!\n"
        }

        for model in defaultModels {
            let fileURL = getDocumentsDirectory().appendingPathComponent(model.filename)
            if FileManager.default.fileExists(atPath: fileURL.path) {

            } else {
                var undownloadedModel = model
                undownloadedModel.status = "download"
                undownloadedModels.append(undownloadedModel)
            }
        }
    }

    func getDocumentsDirectory() -> URL {
        let paths = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)
        return paths[0]
    }
    private let defaultModels: [Model] = [
        Model(name: "TinyLlama-1.1B (Q4_0, 0.6 GiB)",url: "https://huggingface.co/TheBloke/TinyLlama-1.1B-1T-OpenOrca-GGUF/resolve/main/tinyllama-1.1b-1t-openorca.Q4_0.gguf?download=true",filename: "tinyllama-1.1b-1t-openorca.Q4_0.gguf", status: "download"),
        Model(
            name: "TinyLlama-1.1B Chat (Q8_0, 1.1 GiB)",
            url: "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf?download=true",
            filename: "tinyllama-1.1b-chat-v1.0.Q8_0.gguf", status: "download"
        ),

        Model(
            name: "TinyLlama-1.1B (F16, 2.2 GiB)",
            url: "https://huggingface.co/ggml-org/models/resolve/main/tinyllama-1.1b/ggml-model-f16.gguf?download=true",
            filename: "tinyllama-1.1b-f16.gguf", status: "download"
        ),

        Model(
            name: "Phi-2.7B (Q4_0, 1.6 GiB)",
            url: "https://huggingface.co/ggml-org/models/resolve/main/phi-2/ggml-model-q4_0.gguf?download=true",
            filename: "phi-2-q4_0.gguf", status: "download"
        ),

        Model(
            name: "Phi-2.7B (Q8_0, 2.8 GiB)",
            url: "https://huggingface.co/ggml-org/models/resolve/main/phi-2/ggml-model-q8_0.gguf?download=true",
            filename: "phi-2-q8_0.gguf", status: "download"
        ),

        Model(
            name: "Mistral-7B-v0.1 (Q4_0, 3.8 GiB)",
            url: "https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_0.gguf?download=true",
            filename: "mistral-7b-v0.1.Q4_0.gguf", status: "download"
        ),
        Model(
            name: "OpenHermes-2.5-Mistral-7B (Q3_K_M, 3.52 GiB)",
            url: "https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q3_K_M.gguf?download=true",
            filename: "openhermes-2.5-mistral-7b.Q3_K_M.gguf", status: "download"
        )
    ]
    func loadModel(modelUrl: URL?) throws {
        if let modelUrl {
            #if DEBUG
            print("üîÑ [LlamaState] Loading model from: \(modelUrl.path)")
            #endif
            messageLog += "Loading model...\n"
            SystemLog().logEvent(event: "[LlamaState] Loading model: \(modelUrl.lastPathComponent)")

            llamaContext = try LlamaContext.create_context(path: modelUrl.path)

            #if DEBUG
            print("‚úÖ [LlamaState] Model loaded successfully: \(modelUrl.lastPathComponent)")
            #endif
            messageLog += "Loaded model \(modelUrl.lastPathComponent)\n"
            SystemLog().logEvent(event: "[LlamaState] Model loaded: \(modelUrl.lastPathComponent)")

            // „É¢„Éá„É´/Áí∞Â¢ÉÊÉÖÂ†±„Çí„É≠„Ç∞
            if let llamaContext {
                Task { [weak self] in
                    let info = await llamaContext.system_info()
                    SystemLog().logEvent(event: "[llama] system_info: \(info)")
                    #if DEBUG
                    print("‚ÑπÔ∏è [LlamaState] System info: \(info)")
                    #endif
                    await MainActor.run { self?.messageLog += "system_info captured\n" }
                }
            }

            // „Éó„É™„Çª„ÉÉ„ÉàË®≠ÂÆö„ÇíÂèçÊò†
            Task { [weak self] in
                await self?.applyConfigToContext()
            }

            // Assuming that the model is successfully loaded, update the downloaded models
            updateDownloadedModels(modelName: modelUrl.lastPathComponent, status: "downloaded")
        } else {
            #if DEBUG
            print("‚ö†Ô∏è [LlamaState] No model URL provided")
            #endif
            messageLog += "Load a model from the list below\n"
        }
    }


    private func updateDownloadedModels(modelName: String, status: String) {
        undownloadedModels.removeAll { $0.name == modelName }
    }

    // Ê≠£Ë¶èÂåñ„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Ôºà„É¢„Éá„É´Â∑ÆÁï∞„Å´‰æùÂ≠ò„Åó„Å™„ÅÑÔºâ
    private func normalizeOutput(_ s: String) -> String {
        // 1) Êú™„ÇØ„É≠„Éº„Ç∫Âê´„ÇÄ<think>„ÇíÈô§Âéª
        let withoutThink = s.replacingOccurrences(
            of: "(?is)<think>.*?(</think>|$)",
            with: "",
            options: .regularExpression
        )
        // 2) Âà∂Âæ°„Éà„Éº„ÇØ„É≥„ÅÆÊéÉÈô§
        var t = withoutThink
            .replacingOccurrences(of: "<\\|im_start\\|>assistant", with: "", options: .regularExpression)
            .replacingOccurrences(of: "<\\|im_start\\|>user", with: "", options: .regularExpression)
            .replacingOccurrences(of: "<\\|im_start\\|>system", with: "", options: .regularExpression)
        t = t.components(separatedBy: "<|im_end|>").first ?? t
        t = t.replacingOccurrences(of: "<\\|.*?\\|>", with: "", options: .regularExpression)
        // 3) Ë¶ãÂá∫„ÅóÁöÑ„Å™ "assistant:" „ÇíÈô§Âéª
        t = t.replacingOccurrences(of: "^assistant:?\\s*", with: "", options: [.regularExpression, .caseInsensitive])
        return t.trimmingCharacters(in: .whitespacesAndNewlines)
    }

    func complete(text: String) async -> String {
        guard let llamaContext else {
            #if DEBUG
            print("‚ùå [LlamaState] No llamaContext available!")
            #endif
            SystemLog().logEvent(event: "[LlamaState] ERROR: No llamaContext")
            return ""
        }

        #if DEBUG
        print("üé¨ [LlamaState] Starting completion for prompt length: \(text.count)")
        #endif
        SystemLog().logEvent(event: "[LlamaState] Starting completion, promptLen=\(text.count)")

        // Ë®≠ÂÆö„ÇíÂøµ„ÅÆ„Åü„ÇÅÂèçÊò†ÔºàÁõ¥Ââç„Å´Â§âÊõ¥„Åå„ÅÇ„ÇãÂ†¥ÂêàÔºâ
        await applyConfigToContext()

        let t_start = DispatchTime.now().uptimeNanoseconds
        await llamaContext.completion_init(text: text)
        let t_heat_end = DispatchTime.now().uptimeNanoseconds
        let t_heat = Double(t_heat_end - t_start) / NS_PER_S

        #if DEBUG
        print("üî• [LlamaState] Heat up completed in \(String(format: "%.2f", t_heat))s")
        #endif

        await MainActor.run {
            self.messageLog += "USER: \(text)\n"
        }

        var assistantResponse = ""
        // „Çπ„Éà„É™„Éº„É†„Éï„Ç£„É´„ÇøÁî®„Éê„ÉÉ„Éï„Ç°„Å®Áä∂ÊÖã
        var buffer = ""
        var inThink = false
        var thinkChars: Int = 0
        var thinkStartNS: UInt64? = nil
        let THINK_TIMEOUT_S: Double = 3.0 // iOS„Åß„ÅÆ„Çπ„Çø„ÉÉ„ÇØÈò≤Ê≠¢
        let THINK_CHAR_LIMIT = 4000       // Èï∑„Åô„Åé„ÇãÊÄùËÄÉ„ÅØÊâì„Å°Âàá„Çã
        // ÂÖ®‰Ωì„Ç¶„Ç©„ÉÉ„ÉÅ„Éâ„ÉÉ„Ç∞
        let GENERATION_TIMEOUT_S: Double = 20.0
        let genStartNS = DispatchTime.now().uptimeNanoseconds

        #if DEBUG
        var tokenCount = 0
        var loopCount = 0
        #endif

        while await !llamaContext.is_done {
            #if DEBUG
            loopCount += 1
            #endif

            // ÂÖ®‰Ωì„Çø„Ç§„É†„Ç¢„Ç¶„Éà
            let genElapsed = Double(DispatchTime.now().uptimeNanoseconds - genStartNS) / NS_PER_S
            if genElapsed > GENERATION_TIMEOUT_S {
                #if DEBUG
                print("‚è±Ô∏è [LlamaState] Generation timeout after \(String(format: "%.2f", genElapsed))s")
                #endif
                await llamaContext.request_stop()
                break
            }

            // Check for stuck generation (no tokens after many loops)
            #if DEBUG
            if loopCount > 50 && tokenCount == 0 {
                print("‚ö†Ô∏è [LlamaState] WARNING: \(loopCount) loops but no tokens yet!")
            }
            if loopCount > 100 && tokenCount == 0 {
                print("‚ùå [LlamaState] ERROR: Generation stuck - 100 loops with 0 tokens")
                SystemLog().logEvent(event: "[LlamaState] ERROR: Generation stuck after 100 loops")
                await llamaContext.request_stop()
                break
            }
            #endif

            let chunk = await llamaContext.completion_loop()
            if chunk.isEmpty { continue }

            #if DEBUG
            tokenCount += 1
            if tokenCount == 1 {
                print("üéâ [LlamaState] First token received!")
                SystemLog().logEvent(event: "[LlamaState] First token generated after \(loopCount) loops")
            }
            if tokenCount % 10 == 0 {
                print("üìä [LlamaState] Generated \(tokenCount) tokens...")
            }
            #endif

            buffer += chunk

            // Stop„Éà„Éº„ÇØ„É≥: <|im_end|> „ÅßÂç≥ÁµÇ‰∫Ü
            if let endIdx = buffer.range(of: "<|im_end|>") {
                let prefix = String(buffer[..<endIdx.lowerBound])
                if !prefix.isEmpty { assistantResponse += prefix }
                await llamaContext.request_stop()
                break
            }

            // streaming„Åß<think>‚Ä¶</think>„ÇíÈô§Âéª
            processing: while true {
                if inThink {
                    if let rng = buffer.range(of: "</think>") {
                        let after = buffer[rng.upperBound...]
                        buffer = String(after)
                        inThink = false
                        thinkChars = 0
                        thinkStartNS = nil
                        continue processing
                    } else {
                        thinkChars += buffer.count
                        if thinkStartNS == nil { thinkStartNS = DispatchTime.now().uptimeNanoseconds }
                        let elapsed = (Double((DispatchTime.now().uptimeNanoseconds) - (thinkStartNS ?? 0)) / NS_PER_S)
                        if (elapsed > THINK_TIMEOUT_S || thinkChars > THINK_CHAR_LIMIT) {
                            // ÊÄùËÄÉ„Éñ„É≠„ÉÉ„ÇØ„ÅåÈñâ„Åò„Å™„ÅÑ -> Âº∑Âà∂ÂÅúÊ≠¢
                            await llamaContext.request_stop()
                            buffer.removeAll(keepingCapacity: true)
                            break
                        }
                        // Á∂ôÁ∂öÂæÖ„Å°
                        break processing
                    }
                } else {
                    if let rng = buffer.range(of: "<think>") {
                        let prefix = String(buffer[..<rng.lowerBound])
                        if !prefix.isEmpty { assistantResponse += prefix }
                        buffer = String(buffer[rng.upperBound...])
                        inThink = true
                        thinkChars = 0
                        thinkStartNS = DispatchTime.now().uptimeNanoseconds
                        continue processing
                    } else {
                        assistantResponse += buffer
                        buffer.removeAll(keepingCapacity: true)
                        break processing
                    }
                }
            }
        }

        let t_end = DispatchTime.now().uptimeNanoseconds
        let t_generation = Double(t_end - t_heat_end) / self.NS_PER_S
        let tokens_per_second = Double(await llamaContext.n_len) / t_generation

        #if DEBUG
        print("‚è±Ô∏è [LlamaState] Generation completed in \(String(format: "%.2f", t_generation))s")
        print("‚ö° [LlamaState] Speed: \(String(format: "%.2f", tokens_per_second)) tokens/s")
        print("üìä [LlamaState] Total tokens: \(tokenCount)")
        print("üìù [LlamaState] Raw response length: \(assistantResponse.count)")
        #endif

        await llamaContext.clear()

        // ÊúÄÁµÇÊ≠£Ë¶èÂåñ
        let finalAnswer = normalizeOutput(assistantResponse)

        #if DEBUG
        print("‚ú® [LlamaState] Normalized response length: \(finalAnswer.count)")
        print("üìä [LlamaState] Token metrics: \(tokenCount) tokens in \(loopCount) loops")
        if finalAnswer.isEmpty {
            print("‚ö†Ô∏è [LlamaState] WARNING: Final answer is empty!")
            if tokenCount == 0 {
                print("‚ùå [LlamaState] ERROR: No tokens were generated!")
            }
        }
        #endif
        SystemLog().logEvent(event: "[LlamaState] Generation complete: \(finalAnswer.count) chars, \(tokenCount) tokens, \(String(format: "%.2f", tokens_per_second)) t/s")

        // Explicit check for empty generation
        if finalAnswer.isEmpty && tokenCount == 0 {
            let errorMsg = "No tokens generated - model may be incompatible or context initialization failed"
            #if DEBUG
            print("‚ùå [LlamaState] \(errorMsg)")
            #endif
            SystemLog().logEvent(event: "[LlamaState] ERROR: \(errorMsg)")
        }

        await MainActor.run {
            self.messageLog += "ASSISTANT: \(finalAnswer)\n"
            self.messageLog += "\nDone\nHeat up took \(t_heat)s\nGenerated \(tokens_per_second) t/s\n"
        }
        return finalAnswer
    }

    func bench() async {
        guard let llamaContext else {
            return
        }

        messageLog += "\n"
        messageLog += "Running benchmark...\n"
        messageLog += "Model info: "
        messageLog += await llamaContext.model_info() + "\n"

        let t_start = DispatchTime.now().uptimeNanoseconds
        let _ = await llamaContext.bench(pp: 8, tg: 4, pl: 1) // heat up
        let t_end = DispatchTime.now().uptimeNanoseconds

        let t_heat = Double(t_end - t_start) / NS_PER_S
        messageLog += "Heat up time: \(t_heat) seconds, please wait...\n"

        // if more than 5 seconds, then we're probably running on a slow device
        if t_heat > 5.0 {
            messageLog += "Heat up time is too long, aborting benchmark\n"
            return
        }

        let result = await llamaContext.bench(pp: 512, tg: 128, pl: 1, nr: 3)

        messageLog += "\(result)"
        messageLog += "\n"
    }

    func clear() async {
        guard let llamaContext else {
            return
        }

        await llamaContext.clear()
        messageLog = ""
    }
}
